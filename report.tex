\documentclass[12pt, a4paper]{article}

% ===== Packages =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{caption}

\geometry{margin=1in}
\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}

% ===== Code Listing Style =====
\lstdefinestyle{pythonstyle}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{gray}\itshape,
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    tabsize=4,
    showstringspaces=false
}

% ===== Header / Footer =====
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Sentiment Analysis Using Machine Learning}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ===== Title =====
\title{
    \vspace{-1cm}
    \textbf{Sentiment Analysis Using Machine Learning} \\[0.3cm]
    \large Python | NLP | scikit-learn
}
\author{Project Report}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{fancy}

\tableofcontents
\newpage

% =========================================================================
\section{Introduction}
% =========================================================================

Sentiment analysis is a Natural Language Processing (NLP) technique used to determine the emotional tone behind a body of text. It is widely used in social media monitoring, customer feedback analysis, brand reputation management, and market research.

This project implements a complete sentiment analysis pipeline that classifies text (tweets) into \textbf{Positive} or \textbf{Negative} sentiment categories. The system includes data preprocessing, feature extraction, model training, evaluation, a command-line interface (CLI) for predictions, and an interactive web application built with Streamlit.

\subsection{Objectives}
\begin{itemize}
    \item Load and clean raw text data (tweets)
    \item Preprocess text using NLP techniques (stopword removal, lemmatization)
    \item Extract features using TF-IDF Vectorization (unigrams + bigrams)
    \item Train and evaluate a Machine Learning classifier
    \item Build a CLI tool and a web app for real-time sentiment prediction
    \item Save and reuse the trained model using serialization
\end{itemize}

% =========================================================================
\section{Dataset}
% =========================================================================

\subsection{Source}
The dataset used is the \textbf{Sentiment140} dataset, a widely used benchmark dataset for sentiment analysis research. It was originally collected by Stanford University researchers.

\begin{itemize}
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/kazanova/sentiment140}
    \item \textbf{Total Records:} 1,600,000 tweets
    \item \textbf{Training Sample Used:} 100,000 tweets (for computational efficiency)
    \item \textbf{Format:} CSV (Comma Separated Values)
    \item \textbf{Encoding:} Latin-1 (ISO 8859-1)
\end{itemize}

\subsection{Dataset Structure}
The raw CSV file contains 6 columns with no header row:

\begin{table}[h!]
\centering
\caption{Dataset Column Description}
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Index} & \textbf{Column} & \textbf{Type} & \textbf{Description} \\
\midrule
0 & target   & Integer & Sentiment label (0 = Negative, 4 = Positive) \\
1 & id       & Integer & Unique tweet ID \\
2 & date     & String  & Timestamp of the tweet \\
3 & flag     & String  & Query flag (NO\_QUERY if not applicable) \\
4 & user     & String  & Username of the tweet author \\
5 & text     & String  & The raw tweet text \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Label Mapping}
The original dataset uses the following labels:
\begin{itemize}
    \item \textbf{0} $\rightarrow$ Negative
    \item \textbf{4} $\rightarrow$ Positive
\end{itemize}

For binary classification, label \texttt{4} is remapped to \texttt{1}:
\begin{itemize}
    \item \textbf{0} $\rightarrow$ Negative
    \item \textbf{1} $\rightarrow$ Positive
\end{itemize}

The dataset is approximately balanced with an equal number of positive and negative tweets.

% =========================================================================
\section{Tech Stack and Libraries}
% =========================================================================

\begin{table}[h!]
\centering
\caption{Libraries and Their Purpose}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Library} & \textbf{Version} & \textbf{Purpose} \\
\midrule
Python       & 3.x    & Core programming language \\
pandas       & latest & Data loading and manipulation \\
scikit-learn & latest & ML pipeline (TF-IDF, Logistic Regression, metrics) \\
NLTK         & latest & NLP preprocessing (stopwords, lemmatization) \\
Matplotlib   & latest & Confusion matrix visualization \\
Joblib       & latest & Model serialization (save/load) \\
Streamlit    & latest & Interactive web application \\
\bottomrule
\end{tabular}
\end{table}

% =========================================================================
\section{Project Structure}
% =========================================================================

\begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
Sentiment-Analysis-ML/
|
|-- data/
|   |-- sentiment.csv            # Dataset file (Sentiment140)
|
|-- src/
|   |-- preprocess.py            # Text cleaning, lemmatization, data loading
|   |-- train.py                 # Model training, evaluation, confusion matrix
|   |-- predict.py               # CLI prediction script
|
|-- models/
|   |-- sentiment_model.pkl      # Saved trained model
|   |-- confusion_matrix.png     # Confusion matrix visualization
|
|-- app.py                       # Streamlit web application
|-- main.py                      # Simple project runner
|-- requirements.txt             # Python dependencies
|-- report.tex                   # This report (LaTeX source)
|-- README.md                    # GitHub documentation
|-- .gitignore                   # Git ignore rules
\end{lstlisting}

% =========================================================================
\section{Methodology}
% =========================================================================

The project follows a standard Machine Learning pipeline:

\begin{center}
\texttt{Raw Data} $\rightarrow$ \texttt{Preprocessing} $\rightarrow$ \texttt{Feature Extraction} $\rightarrow$ \texttt{Model Training} $\rightarrow$ \texttt{Evaluation} $\rightarrow$ \texttt{Prediction}
\end{center}

% ----- 5.1 -----
\subsection{Data Loading}
The dataset is loaded using \texttt{pandas.read\_csv()} with \texttt{latin-1} encoding. Only the \texttt{target} (label) and \texttt{text} (tweet) columns are retained. The positive label is remapped from 4 to 1. A random sample of 100,000 rows is drawn for training efficiency.

% ----- 5.2 -----
\subsection{Text Preprocessing}
Each tweet undergoes the following cleaning steps (implemented in \texttt{src/preprocess.py}):

\begin{enumerate}
    \item \textbf{Lowercasing:} Convert all text to lowercase for uniformity.
    \item \textbf{URL Removal:} Remove all HTTP/HTTPS links using regex.
    \item \textbf{Mention \& Hashtag Removal:} Strip Twitter-specific \texttt{@user} mentions and \texttt{\#} symbols.
    \item \textbf{Punctuation Removal:} Remove all non-alphanumeric characters except spaces.
    \item \textbf{Number Removal:} Remove all numeric digits.
    \item \textbf{Stopword Removal:} Remove common English words (``the'', ``is'', ``at'', etc.) using NLTK's stopword corpus.
    \item \textbf{Lemmatization:} Reduce words to their base/root form using NLTK's WordNet Lemmatizer (e.g., ``running'' $\rightarrow$ ``run'', ``better'' $\rightarrow$ ``better'').
    \item \textbf{Short Word Removal:} Remove single-character tokens.
\end{enumerate}

\textbf{Example:}
\begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
Input:  "@user I am SO happy!!! Check http://example.com #love"
Output: "happy check love"
\end{lstlisting}

% ----- 5.3 -----
\subsection{Feature Extraction: TF-IDF Vectorization}
Text data is converted into numerical feature vectors using \textbf{Term Frequency--Inverse Document Frequency (TF-IDF)}.

The TF-IDF score for a term $t$ in document $d$ from corpus $D$ is calculated as:

$$\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)$$

Where:
$$\text{TF}(t, d) = \frac{\text{Number of times } t \text{ appears in } d}{\text{Total number of terms in } d}$$

$$\text{IDF}(t, D) = \log\left(\frac{|D|}{|\{d \in D : t \in d\}|}\right)$$

\textbf{Configuration:}
\begin{itemize}
    \item \texttt{max\_features = 10,000} — Top 10,000 most important features
    \item \texttt{ngram\_range = (1, 2)} — Both unigrams (single words) and bigrams (word pairs)
\end{itemize}

Bigrams are important because they capture phrases like ``not good'' or ``very bad'' which carry different meaning than individual words.

% ----- 5.4 -----
\subsection{Model: Logistic Regression}
\textbf{Logistic Regression} is a linear classification algorithm that models the probability of a binary outcome. It is well-suited for text classification due to its:
\begin{itemize}
    \item Fast training and prediction speed
    \item Good performance on high-dimensional sparse data (like TF-IDF vectors)
    \item Interpretability
    \item Ability to output probability scores
\end{itemize}

The model computes the probability using the sigmoid function:
$$P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n)}}$$

\textbf{Configuration:}
\begin{itemize}
    \item \texttt{max\_iter = 1000} — Maximum iterations for convergence
    \item \texttt{random\_state = 42} — For reproducibility
\end{itemize}

The entire pipeline (TF-IDF + Logistic Regression) is encapsulated in a \texttt{sklearn.pipeline.Pipeline} for clean, reproducible execution.

% ----- 5.5 -----
\subsection{Train/Test Split}
The data is split into training and testing sets:
\begin{itemize}
    \item \textbf{Training Set:} 80\% (80,000 tweets)
    \item \textbf{Testing Set:} 20\% (20,000 tweets)
    \item \textbf{Random State:} 42 (for reproducibility)
\end{itemize}

% =========================================================================
\section{Results and Evaluation}
% =========================================================================

\subsection{Accuracy}
The trained model achieved an overall accuracy of:

$$\boxed{\text{Accuracy} = 76.95\%}$$

\subsection{Classification Report}

\begin{table}[h!]
\centering
\caption{Classification Report}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
Negative & 0.77 & 0.75 & 0.76 & 9,822 \\
Positive & 0.77 & 0.78 & 0.78 & 10,085 \\
\midrule
\textbf{Accuracy}      & \multicolumn{3}{c}{}       & \textbf{0.77} \\
\textbf{Macro Avg}     & 0.77 & 0.77 & 0.77 & 19,907 \\
\textbf{Weighted Avg}  & 0.77 & 0.77 & 0.77 & 19,907 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Metrics Explanation}
\begin{itemize}
    \item \textbf{Precision:} Of all tweets predicted as a class, what fraction was correct?
    $$\text{Precision} = \frac{TP}{TP + FP}$$
    \item \textbf{Recall:} Of all actual tweets of a class, what fraction was correctly identified?
    $$\text{Recall} = \frac{TP}{TP + FN}$$
    \item \textbf{F1-Score:} Harmonic mean of Precision and Recall.
    $$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$
\end{itemize}

\subsection{Confusion Matrix}
The confusion matrix visualizes correct and incorrect predictions:

\begin{figure}[h!]
\centering
\includegraphics[width=0.6\textwidth]{models/confusion_matrix.png}
\caption{Confusion Matrix of the Logistic Regression Model}
\end{figure}

The diagonal elements represent correct predictions (True Positives and True Negatives), while the off-diagonal elements represent misclassifications.

% =========================================================================
\section{Model Serialization}
% =========================================================================

The trained pipeline (TF-IDF vectorizer + Logistic Regression model) is saved to disk using \texttt{joblib}:

\begin{lstlisting}[style=pythonstyle]
import joblib
joblib.dump(pipeline, 'models/sentiment_model.pkl')
\end{lstlisting}

This allows the model to be loaded later without retraining:

\begin{lstlisting}[style=pythonstyle]
model = joblib.load('models/sentiment_model.pkl')
prediction = model.predict(["I love this!"])
\end{lstlisting}

% =========================================================================
\section{Application Interfaces}
% =========================================================================

\subsection{Command-Line Interface (CLI)}
The file \texttt{src/predict.py} provides an interactive CLI where users can type text and receive instant sentiment predictions:

\begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
$ python src/predict.py

--- Sentiment Analysis CLI ---
Model loaded successfully!
Enter text to analyze (type 'exit' to quit):

Input: I love this movie so much
Sentiment: Positive

Input: This is the worst experience ever
Sentiment: Negative
\end{lstlisting}

\subsection{Streamlit Web Application}
The file \texttt{app.py} provides a browser-based web interface with:
\begin{itemize}
    \item \textbf{Single Text Analysis:} Enter one sentence and get sentiment + confidence score
    \item \textbf{Batch Analysis:} Enter multiple sentences (one per line) and analyze all at once
    \item \textbf{Sidebar:} Displays model information and the confusion matrix
\end{itemize}

To run the web app:
\begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
$ python -m streamlit run app.py
\end{lstlisting}

The app opens in the browser at \texttt{http://localhost:8501}.

% =========================================================================
\section{How to Run the Project}
% =========================================================================

\begin{enumerate}
    \item \textbf{Clone the repository:}
    \begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
git clone https://github.com/<username>/Sentiment-Analysis-ML.git
cd Sentiment-Analysis-ML
    \end{lstlisting}

    \item \textbf{Install dependencies:}
    \begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
pip install -r requirements.txt
    \end{lstlisting}

    \item \textbf{Train the model:}
    \begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
python src/train.py
    \end{lstlisting}

    \item \textbf{Run CLI predictions:}
    \begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
python src/predict.py
    \end{lstlisting}

    \item \textbf{Run the web app:}
    \begin{lstlisting}[style=pythonstyle, language={}, numbers=none]
python -m streamlit run app.py
    \end{lstlisting}
\end{enumerate}

% =========================================================================
\section{Improvements Made}
% =========================================================================

During development, the following improvements were applied to boost performance:

\begin{table}[h!]
\centering
\caption{Improvements and Impact}
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{\#} & \textbf{Improvement} & \textbf{Before} & \textbf{After} \\
\midrule
1 & Naive Bayes $\rightarrow$ Logistic Regression & 73.87\% & 76.95\% \\
2 & Added Lemmatization & -- & Cleaner features \\
3 & Added Bigrams to TF-IDF & unigrams only & unigrams + bigrams \\
4 & Increased training data (50K $\rightarrow$ 100K) & 50,000 & 100,000 \\
5 & Increased TF-IDF features (5K $\rightarrow$ 10K) & 5,000 & 10,000 \\
6 & Added Confusion Matrix visualization & -- & \checkmark \\
7 & Added Streamlit Web App & CLI only & CLI + Web App \\
\bottomrule
\end{tabular}
\end{table}

% =========================================================================
\section{Future Scope}
% =========================================================================

\begin{itemize}
    \item Train on the full 1.6 million tweet dataset for higher accuracy
    \item Experiment with SVM or Random Forest classifiers
    \item Use word embeddings (Word2Vec, GloVe) instead of TF-IDF
    \item Implement deep learning models (LSTM, BERT) for state-of-the-art performance
    \item Add neutral sentiment class for three-way classification
    \item Deploy the Streamlit app to Streamlit Cloud or Heroku
    \item Add real-time Twitter API integration for live sentiment tracking
\end{itemize}

% =========================================================================
\section{Conclusion}
% =========================================================================

This project successfully demonstrates a complete sentiment analysis pipeline from raw data to a deployable application. By using NLP preprocessing techniques (lemmatization, stopword removal), TF-IDF feature extraction with bigrams, and Logistic Regression classification, the model achieves \textbf{76.95\% accuracy} on the Sentiment140 Twitter dataset.

The project includes both a command-line interface and an interactive Streamlit web application, making it practical for real-world use and portfolio presentation.

% =========================================================================
\section*{References}
% =========================================================================

\begin{enumerate}
    \item Go, A., Bhayani, R., \& Huang, L. (2009). \textit{Twitter Sentiment Classification using Distant Supervision.} Stanford University.
    \item scikit-learn Documentation: \url{https://scikit-learn.org/stable/}
    \item NLTK Documentation: \url{https://www.nltk.org/}
    \item Streamlit Documentation: \url{https://docs.streamlit.io/}
    \item Sentiment140 Dataset: \url{https://www.kaggle.com/datasets/kazanova/sentiment140}
\end{enumerate}

\end{document}
